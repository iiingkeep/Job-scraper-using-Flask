'''
ğŸ§‘â€ğŸ’» Static Website Scraping ğŸ§‘â€ğŸ’»
url = https://weworkremotely.com/remote-full-time-jobs#job-listings
ëª©í‘œ = êµ¬ì¸ ì‚¬ì´íŠ¸ì¸ weworkremotelyì˜ full-time ì¹´í…Œê³ ë¦¬ì— ì˜¬ë¼ì˜¨ ëª¨ë“  êµ¬ì¸ê¸€ì—ì„œ íšŒì‚¬ëª…, ì§ë¬´ëª…, full-time ì—¬ë¶€, ê·¼ë¬´ì§€, url ì¶”ì¶œ

scraping í•  ë¶€ë¶„
1. section íƒœê·¸ ì•ˆì˜ ëª¨ë“  li
2. li ì¤‘ í•„ìš” ì—†ëŠ” ë°ì´í„°ë¥¼ ë‹´ê³  ìˆëŠ” ì²«ë²ˆì§¸ì™€ ë§ˆì§€ë§‰ li ì œì™¸
3. li íƒœê·¸ ì•ˆì˜ a íƒœê·¸ ì•ˆì˜ span íƒœê·¸ë“¤ ì¤‘ íšŒì‚¬ ì´ë¦„, ì§ë¬´ëª…, Full-time ì—¬ë¶€, ê·¼ë¬´ì§€ ì¶”ì¶œ
4. í˜ì´ì§€ ê°œìˆ˜ë¥¼ êµ¬í•´ ëª¨ë“  í˜ì´ì§€ì—ì„œ ì‹¤í–‰

<section class="jobs">...</section>
<li class="feature">...</li>
<a href="/remote-jobs/playfly-need-senior-blockchain-engineer-smart-contract-web3-experts">...</a>
<span class="company">Playfly</span><br>
<span class="title">Need Senior Blockchain Engineer - Smart contract &amp; Web3 Experts</span>
<span class="featured">featured</span><br>
<span class="company">Full-Time</span>
<span>/</span>
<span class="region company">Anywhere in the World</span>
'''


import requests
from bs4 import BeautifulSoup

all_jobs = []

# í•´ë‹¹ í˜ì´ì§€ì—ì„œ íšŒì‚¬ëª…, ì§ë¬´, Full-time ì—¬ë¶€, ê·¼ë¬´ì§€ë¥¼ ì¶”ì¶œí•´ job_dataë¼ëŠ” ë”•ì…”ë„ˆë¦¬ë¥¼ ìƒì„±í•˜ê³ , ê°ê°ì˜ ë”•ì…”ë„ˆë¦¬ë¥¼ all-jobs ë¦¬ìŠ¤íŠ¸ì— ë„£ëŠ” í•¨ìˆ˜
def scrape_page(url):
  print(f"Scraping {url} \n")
  response = requests.get(url)

  # response.content: ê·¸ urlì˜ html resourceê°€ ë‚˜ì˜¨ë‹¤.
  # BeautifulSoup: HTML/XML resourceë¥¼ íŒŒì‹±í•´ì£¼ëŠ” ë¼ì´ë¸ŒëŸ¬ë¦¬. 
  # soup = BeautifulSoup(resource, option)
  soup = BeautifulSoup(response.content, 'html.parser')

  # 1+2. sectioníƒœê·¸ ì•ˆì—ì„œ ì²«ë²ˆì§¸ì™€ ë§ˆì§€ë§‰ì„ ì œì™¸í•œ ëª¨ë“  li ê°€ì ¸ì˜¤ê¸°
  jobs = soup.find('section', class_ = 'jobs').find_all('li')[1:-1]
  # classëŠ” ì˜ˆì•½ì–´ì´ê¸° ë•Œë¬¸ì— class_ë¡œ ì‚¬ìš©í•´ì•¼í•œë‹¤.

  # 3. li íƒœê·¸ ì•ˆì— ìˆëŠ” span íƒœê·¸ì—ì„œ classnameì„ í†µí•´ ì§ë¬´, íšŒì‚¬ëª…, í¬ì§€ì…˜, ì§€ì—­ ë°ì´í„° ì¶”ì¶œ
  for job in jobs:
    title = job.find('span', class_= 'title').text
    try:
      url = job.find('div', class_ = 'tooltip--flag-logo').next_sibling['href']
    except KeyError:
      url = 'You need log-in'
    company, position, region = job.find_all('span',   class_ = 'company')[0:3]
    job_data = {
      "title": title,
      "company": company.text,
      "position": position.text,
      "region": region.text,
      "url": f"https://weworkremotely.com{url}"
    }
    all_jobs.append(job_data)
  print(f"Number of tasks done: {len(all_jobs)} \n")

# í˜ì´ì§€ì˜ ê°¯ìˆ˜ë¥¼ êµ¬í•˜ëŠ” í•¨ìˆ˜
def get_pages(url):
  response = requests.get(url)
  soup = BeautifulSoup(response.content, 'html.parser')
  return len(soup.find('div', class_ = 'pagination').find_all('span', class_ = 'page'))
  

total_pages = get_pages('https://weworkremotely.com/remote-full-time-jobs?page=1')

# ëª¨ë“  í˜ì´ì§€ scraping
for x in range(total_pages):
  url = f"https://weworkremotely.com/remote-full-time-jobs?page={x+1}"
  scrape_page(url)

print(f"Total number of posts: {len(all_jobs)}")